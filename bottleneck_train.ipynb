{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import optimizers\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "\n",
    "from imagenet_utils import preprocess_input\n",
    "from input_preparation_new import*\n",
    "\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 200, 200\n",
    "train_data_dir = \"images/train\"\n",
    "validation_data_dir = \"images/dev\"\n",
    "nb_train_samples = 32488\n",
    "nb_validation_samples = 5723\n",
    "batch_size = 32\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/ResNet50_trained_f3d.h5')\n",
    "flatten_model = Model(inputs=model.input, outputs=model.get_layer('flatten').output)\n",
    "flatten_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "img_generator = img_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False,\n",
    "        label_name='label_img_train.h5')\n",
    "\n",
    "bottleneck_img_train = flatten_model.predict_generator(img_generator)\n",
    "print(bottleneck_img_train.shape)\n",
    "np.save('bottleneck_img_train.npy', bottleneck_img_train)\n",
    "\n",
    "img_generator = img_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(img_height, img_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False,\n",
    "        label_name='label_img_val.h5')\n",
    "\n",
    "bottleneck_img_val = flatten_model.predict_generator(img_generator)\n",
    "print(bottleneck_img_val.shape)\n",
    "np.save('bottleneck_img_val.npy', bottleneck_img_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/feature_network.h5')\n",
    "flatten_model = Model(inputs=model.input, outputs=model.get_layer('dropout_2').output)\n",
    "flatten_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_datagen = ImageDataGenerator()\n",
    "\n",
    "feature_generator = feature_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False,\n",
    "        out_type='feature',\n",
    "        label_name='label_feature_train.h5')\n",
    "\n",
    "bottleneck_feature_train = flatten_model.predict_generator(feature_generator, verbose=1, workers=12, use_multiprocessing=True)\n",
    "print(bottleneck_feature_train.shape)\n",
    "np.save('bottleneck_feature_train.npy', bottleneck_feature_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_generator = feature_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False,\n",
    "        out_type='feature',\n",
    "        label_name='label_feature_val.h5')\n",
    "\n",
    "bottleneck_feature_val = flatten_model.predict_generator(feature_generator, verbose=1, workers=12, use_multiprocessing=True)\n",
    "print(bottleneck_feature_val.shape)\n",
    "np.save('bottleneck_feature_val.npy', bottleneck_feature_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def load_label(name):\n",
    "    label_dataset = h5py.File(name, \"r\")\n",
    "    label = np.array(label_dataset[\"label\"][:])\n",
    "    num_c = np.max(label) + 1\n",
    "    Y = np.eye(num_c)[label.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32488, 3072)\n",
      "(32488, 18)\n",
      "(5723, 3072)\n",
      "(5723, 18)\n"
     ]
    }
   ],
   "source": [
    "train_img = np.load('bottleneck_img_train.npy')\n",
    "train_feature = np.load('bottleneck_feature_train.npy')\n",
    "\n",
    "train_data = np.concatenate((train_img, train_feature), axis=1)\n",
    "train_labels = load_label('label_img_train.h5')\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "val_img = np.load('bottleneck_img_val.npy')\n",
    "val_feature = np.load('bottleneck_feature_val.npy')\n",
    "\n",
    "val_data = np.concatenate((val_img, val_feature), axis=1)\n",
    "val_labels = load_label('label_img_val.h5')\n",
    "print(val_data.shape)\n",
    "print(val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top = Sequential()\n",
    "model_top.add(Dense(18, activation='softmax', input_shape=train_data.shape[1:], name='shopee_output'))\n",
    "\n",
    "model_top.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model according to the conditions\n",
    "checkpoint = ModelCheckpoint(\"models/bottleneck.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=100, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32488 samples, validate on 5723 samples\n",
      "Epoch 1/1000\n",
      "32488/32488 [==============================] - 3s 78us/step - loss: 1.1835 - acc: 0.6694 - val_loss: 0.7533 - val_acc: 0.7744\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.77442, saving model to models/bottleneck.h5\n",
      "Epoch 2/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.5875 - acc: 0.8248 - val_loss: 0.6246 - val_acc: 0.7992\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.77442 to 0.79923, saving model to models/bottleneck.h5\n",
      "Epoch 3/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.4926 - acc: 0.8442 - val_loss: 0.5798 - val_acc: 0.8090\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.79923 to 0.80902, saving model to models/bottleneck.h5\n",
      "Epoch 4/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.4478 - acc: 0.8541 - val_loss: 0.5573 - val_acc: 0.8127\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.80902 to 0.81269, saving model to models/bottleneck.h5\n",
      "Epoch 5/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.4200 - acc: 0.8613 - val_loss: 0.5444 - val_acc: 0.8150\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.81269 to 0.81496, saving model to models/bottleneck.h5\n",
      "Epoch 6/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.4006 - acc: 0.8663 - val_loss: 0.5366 - val_acc: 0.8167\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.81496 to 0.81670, saving model to models/bottleneck.h5\n",
      "Epoch 7/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.3856 - acc: 0.8702 - val_loss: 0.5326 - val_acc: 0.8164\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/1000\n",
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.3737 - acc: 0.8732 - val_loss: 0.5275 - val_acc: 0.8186\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.81670 to 0.81863, saving model to models/bottleneck.h5\n",
      "Epoch 9/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.3641 - acc: 0.8756 - val_loss: 0.5235 - val_acc: 0.8185\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.3557 - acc: 0.8781 - val_loss: 0.5208 - val_acc: 0.8207\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.81863 to 0.82072, saving model to models/bottleneck.h5\n",
      "Epoch 11/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.3485 - acc: 0.8801 - val_loss: 0.5189 - val_acc: 0.8193\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.3421 - acc: 0.8824 - val_loss: 0.5182 - val_acc: 0.8212\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.82072 to 0.82125, saving model to models/bottleneck.h5\n",
      "Epoch 13/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.3363 - acc: 0.8842 - val_loss: 0.5164 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.82125 to 0.82212, saving model to models/bottleneck.h5\n",
      "Epoch 14/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.3312 - acc: 0.8854 - val_loss: 0.5158 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.82212 to 0.82369, saving model to models/bottleneck.h5\n",
      "Epoch 15/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.3266 - acc: 0.8875 - val_loss: 0.5148 - val_acc: 0.8249\n",
      "\n",
      "Epoch 00015: val_acc improved from 0.82369 to 0.82492, saving model to models/bottleneck.h5\n",
      "Epoch 16/1000\n",
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.3222 - acc: 0.8887 - val_loss: 0.5162 - val_acc: 0.8237\n",
      "\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 17/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.3182 - acc: 0.8897 - val_loss: 0.5139 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.82492 to 0.82509, saving model to models/bottleneck.h5\n",
      "Epoch 18/1000\n",
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.3142 - acc: 0.8909 - val_loss: 0.5139 - val_acc: 0.8247\n",
      "\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 19/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.3110 - acc: 0.8910 - val_loss: 0.5139 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 20/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.3079 - acc: 0.8928 - val_loss: 0.5136 - val_acc: 0.8254\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.82509 to 0.82544, saving model to models/bottleneck.h5\n",
      "Epoch 21/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.3048 - acc: 0.8937 - val_loss: 0.5142 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00021: val_acc improved from 0.82544 to 0.82806, saving model to models/bottleneck.h5\n",
      "Epoch 22/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.3018 - acc: 0.8957 - val_loss: 0.5135 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00022: val_acc did not improve\n",
      "Epoch 23/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2991 - acc: 0.8962 - val_loss: 0.5146 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00023: val_acc did not improve\n",
      "Epoch 24/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2965 - acc: 0.8970 - val_loss: 0.5133 - val_acc: 0.8253\n",
      "\n",
      "Epoch 00024: val_acc did not improve\n",
      "Epoch 25/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2939 - acc: 0.8978 - val_loss: 0.5141 - val_acc: 0.8261\n",
      "\n",
      "Epoch 00025: val_acc did not improve\n",
      "Epoch 26/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2917 - acc: 0.8983 - val_loss: 0.5145 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00026: val_acc did not improve\n",
      "Epoch 27/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2894 - acc: 0.8994 - val_loss: 0.5138 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00027: val_acc did not improve\n",
      "Epoch 28/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2873 - acc: 0.9001 - val_loss: 0.5139 - val_acc: 0.8268\n",
      "\n",
      "Epoch 00028: val_acc did not improve\n",
      "Epoch 29/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2851 - acc: 0.9005 - val_loss: 0.5140 - val_acc: 0.8284\n",
      "\n",
      "Epoch 00029: val_acc improved from 0.82806 to 0.82841, saving model to models/bottleneck.h5\n",
      "Epoch 30/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2830 - acc: 0.9014 - val_loss: 0.5143 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00030: val_acc did not improve\n",
      "Epoch 31/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2811 - acc: 0.9015 - val_loss: 0.5140 - val_acc: 0.8258\n",
      "\n",
      "Epoch 00031: val_acc did not improve\n",
      "Epoch 32/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2792 - acc: 0.9027 - val_loss: 0.5143 - val_acc: 0.8261\n",
      "\n",
      "Epoch 00032: val_acc did not improve\n",
      "Epoch 33/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2773 - acc: 0.9029 - val_loss: 0.5144 - val_acc: 0.8254\n",
      "\n",
      "Epoch 00033: val_acc did not improve\n",
      "Epoch 34/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2755 - acc: 0.9039 - val_loss: 0.5173 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00034: val_acc did not improve\n",
      "Epoch 35/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2738 - acc: 0.9045 - val_loss: 0.5156 - val_acc: 0.8253\n",
      "\n",
      "Epoch 00035: val_acc did not improve\n",
      "Epoch 36/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2722 - acc: 0.9045 - val_loss: 0.5162 - val_acc: 0.8253\n",
      "\n",
      "Epoch 00036: val_acc did not improve\n",
      "Epoch 37/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2705 - acc: 0.9051 - val_loss: 0.5152 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00037: val_acc did not improve\n",
      "Epoch 38/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2689 - acc: 0.9058 - val_loss: 0.5173 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00038: val_acc did not improve\n",
      "Epoch 39/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2674 - acc: 0.9069 - val_loss: 0.5172 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00039: val_acc did not improve\n",
      "Epoch 40/1000\n",
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.2661 - acc: 0.9078 - val_loss: 0.5161 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00040: val_acc did not improve\n",
      "Epoch 41/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2644 - acc: 0.9079 - val_loss: 0.5167 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00041: val_acc improved from 0.82841 to 0.82911, saving model to models/bottleneck.h5\n",
      "Epoch 42/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.2631 - acc: 0.9083 - val_loss: 0.5171 - val_acc: 0.8268\n",
      "\n",
      "Epoch 00042: val_acc did not improve\n",
      "Epoch 43/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2619 - acc: 0.9090 - val_loss: 0.5167 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00043: val_acc did not improve\n",
      "Epoch 44/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2604 - acc: 0.9091 - val_loss: 0.5167 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00044: val_acc did not improve\n",
      "Epoch 45/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2590 - acc: 0.9097 - val_loss: 0.5185 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00045: val_acc did not improve\n",
      "Epoch 46/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2577 - acc: 0.9101 - val_loss: 0.5174 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00046: val_acc did not improve\n",
      "Epoch 47/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2564 - acc: 0.9106 - val_loss: 0.5175 - val_acc: 0.8288\n",
      "\n",
      "Epoch 00047: val_acc did not improve\n",
      "Epoch 48/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2550 - acc: 0.9114 - val_loss: 0.5191 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00048: val_acc did not improve\n",
      "Epoch 49/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2539 - acc: 0.9112 - val_loss: 0.5176 - val_acc: 0.8300\n",
      "\n",
      "Epoch 00049: val_acc improved from 0.82911 to 0.82998, saving model to models/bottleneck.h5\n",
      "Epoch 50/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2526 - acc: 0.9118 - val_loss: 0.5187 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00050: val_acc did not improve\n",
      "Epoch 51/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2518 - acc: 0.9125 - val_loss: 0.5189 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00051: val_acc did not improve\n",
      "Epoch 52/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2504 - acc: 0.9126 - val_loss: 0.5196 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00052: val_acc did not improve\n",
      "Epoch 53/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2493 - acc: 0.9131 - val_loss: 0.5186 - val_acc: 0.8284\n",
      "\n",
      "Epoch 00053: val_acc did not improve\n",
      "Epoch 54/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2482 - acc: 0.9141 - val_loss: 0.5199 - val_acc: 0.8272\n",
      "\n",
      "Epoch 00054: val_acc did not improve\n",
      "Epoch 55/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2471 - acc: 0.9144 - val_loss: 0.5210 - val_acc: 0.8286\n",
      "\n",
      "Epoch 00055: val_acc did not improve\n",
      "Epoch 56/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2461 - acc: 0.9145 - val_loss: 0.5207 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00056: val_acc did not improve\n",
      "Epoch 57/1000\n",
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.2451 - acc: 0.9150 - val_loss: 0.5212 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00057: val_acc did not improve\n",
      "Epoch 58/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2439 - acc: 0.9159 - val_loss: 0.5208 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00058: val_acc did not improve\n",
      "Epoch 59/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2430 - acc: 0.9161 - val_loss: 0.5211 - val_acc: 0.8272\n",
      "\n",
      "Epoch 00059: val_acc did not improve\n",
      "Epoch 60/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2419 - acc: 0.9164 - val_loss: 0.5228 - val_acc: 0.8296\n",
      "\n",
      "Epoch 00060: val_acc did not improve\n",
      "Epoch 61/1000\n",
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.2409 - acc: 0.9165 - val_loss: 0.5214 - val_acc: 0.8293\n",
      "\n",
      "Epoch 00061: val_acc did not improve\n",
      "Epoch 62/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2401 - acc: 0.9166 - val_loss: 0.5216 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00062: val_acc did not improve\n",
      "Epoch 63/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2391 - acc: 0.9173 - val_loss: 0.5220 - val_acc: 0.8288\n",
      "\n",
      "Epoch 00063: val_acc did not improve\n",
      "Epoch 64/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2382 - acc: 0.9177 - val_loss: 0.5221 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00064: val_acc did not improve\n",
      "Epoch 65/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2371 - acc: 0.9179 - val_loss: 0.5229 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00065: val_acc did not improve\n",
      "Epoch 66/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2362 - acc: 0.9188 - val_loss: 0.5225 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00066: val_acc did not improve\n",
      "Epoch 67/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2354 - acc: 0.9186 - val_loss: 0.5233 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00067: val_acc did not improve\n",
      "Epoch 68/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2345 - acc: 0.9191 - val_loss: 0.5233 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00068: val_acc did not improve\n",
      "Epoch 69/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2335 - acc: 0.9193 - val_loss: 0.5234 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00069: val_acc did not improve\n",
      "Epoch 70/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2328 - acc: 0.9197 - val_loss: 0.5239 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00070: val_acc did not improve\n",
      "Epoch 71/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2319 - acc: 0.9204 - val_loss: 0.5235 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00071: val_acc did not improve\n",
      "Epoch 72/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2310 - acc: 0.9203 - val_loss: 0.5246 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00072: val_acc did not improve\n",
      "Epoch 73/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2301 - acc: 0.9208 - val_loss: 0.5246 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00073: val_acc did not improve\n",
      "Epoch 74/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2294 - acc: 0.9211 - val_loss: 0.5252 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00074: val_acc did not improve\n",
      "Epoch 75/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2286 - acc: 0.9216 - val_loss: 0.5254 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00075: val_acc did not improve\n",
      "Epoch 76/1000\n",
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.2277 - acc: 0.9215 - val_loss: 0.5250 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00076: val_acc did not improve\n",
      "Epoch 77/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2270 - acc: 0.9217 - val_loss: 0.5261 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00077: val_acc did not improve\n",
      "Epoch 78/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2262 - acc: 0.9219 - val_loss: 0.5256 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00078: val_acc did not improve\n",
      "Epoch 79/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2254 - acc: 0.9218 - val_loss: 0.5262 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00079: val_acc did not improve\n",
      "Epoch 80/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2248 - acc: 0.9225 - val_loss: 0.5262 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00080: val_acc did not improve\n",
      "Epoch 81/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2240 - acc: 0.9231 - val_loss: 0.5260 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00081: val_acc did not improve\n",
      "Epoch 82/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2230 - acc: 0.9235 - val_loss: 0.5270 - val_acc: 0.8268\n",
      "\n",
      "Epoch 00082: val_acc did not improve\n",
      "Epoch 83/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2225 - acc: 0.9241 - val_loss: 0.5265 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00083: val_acc did not improve\n",
      "Epoch 84/1000\n",
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.2217 - acc: 0.9242 - val_loss: 0.5286 - val_acc: 0.8272\n",
      "\n",
      "Epoch 00084: val_acc did not improve\n",
      "Epoch 85/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2211 - acc: 0.9244 - val_loss: 0.5269 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00085: val_acc did not improve\n",
      "Epoch 86/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2203 - acc: 0.9246 - val_loss: 0.5276 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00086: val_acc did not improve\n",
      "Epoch 87/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2195 - acc: 0.9251 - val_loss: 0.5285 - val_acc: 0.8284\n",
      "\n",
      "Epoch 00087: val_acc did not improve\n",
      "Epoch 88/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2189 - acc: 0.9259 - val_loss: 0.5280 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00088: val_acc did not improve\n",
      "Epoch 89/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2183 - acc: 0.9256 - val_loss: 0.5289 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00089: val_acc did not improve\n",
      "Epoch 90/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2175 - acc: 0.9258 - val_loss: 0.5289 - val_acc: 0.8288\n",
      "\n",
      "Epoch 00090: val_acc did not improve\n",
      "Epoch 91/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2168 - acc: 0.9263 - val_loss: 0.5288 - val_acc: 0.8288\n",
      "\n",
      "Epoch 00091: val_acc did not improve\n",
      "Epoch 92/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2163 - acc: 0.9268 - val_loss: 0.5298 - val_acc: 0.8291\n",
      "\n",
      "Epoch 00092: val_acc did not improve\n",
      "Epoch 93/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2155 - acc: 0.9264 - val_loss: 0.5289 - val_acc: 0.8286\n",
      "\n",
      "Epoch 00093: val_acc did not improve\n",
      "Epoch 94/1000\n",
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.2149 - acc: 0.9266 - val_loss: 0.5297 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00094: val_acc did not improve\n",
      "Epoch 95/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2143 - acc: 0.9274 - val_loss: 0.5296 - val_acc: 0.8289\n",
      "\n",
      "Epoch 00095: val_acc did not improve\n",
      "Epoch 96/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2135 - acc: 0.9272 - val_loss: 0.5304 - val_acc: 0.8286\n",
      "\n",
      "Epoch 00096: val_acc did not improve\n",
      "Epoch 97/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2130 - acc: 0.9283 - val_loss: 0.5303 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00097: val_acc did not improve\n",
      "Epoch 98/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2122 - acc: 0.9285 - val_loss: 0.5305 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00098: val_acc did not improve\n",
      "Epoch 99/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2117 - acc: 0.9287 - val_loss: 0.5321 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00099: val_acc did not improve\n",
      "Epoch 100/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2110 - acc: 0.9287 - val_loss: 0.5323 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00100: val_acc did not improve\n",
      "Epoch 101/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2105 - acc: 0.9293 - val_loss: 0.5311 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00101: val_acc did not improve\n",
      "Epoch 102/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2098 - acc: 0.9293 - val_loss: 0.5333 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00102: val_acc did not improve\n",
      "Epoch 103/1000\n",
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.2093 - acc: 0.9294 - val_loss: 0.5316 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00103: val_acc did not improve\n",
      "Epoch 104/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2088 - acc: 0.9293 - val_loss: 0.5330 - val_acc: 0.8272\n",
      "\n",
      "Epoch 00104: val_acc did not improve\n",
      "Epoch 105/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2080 - acc: 0.9297 - val_loss: 0.5341 - val_acc: 0.8261\n",
      "\n",
      "Epoch 00105: val_acc did not improve\n",
      "Epoch 106/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2076 - acc: 0.9302 - val_loss: 0.5322 - val_acc: 0.8265\n",
      "\n",
      "Epoch 00106: val_acc did not improve\n",
      "Epoch 107/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2069 - acc: 0.9296 - val_loss: 0.5325 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00107: val_acc did not improve\n",
      "Epoch 108/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2064 - acc: 0.9308 - val_loss: 0.5333 - val_acc: 0.8261\n",
      "\n",
      "Epoch 00108: val_acc did not improve\n",
      "Epoch 109/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2058 - acc: 0.9307 - val_loss: 0.5339 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00109: val_acc did not improve\n",
      "Epoch 110/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2053 - acc: 0.9317 - val_loss: 0.5340 - val_acc: 0.8272\n",
      "\n",
      "Epoch 00110: val_acc did not improve\n",
      "Epoch 111/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2047 - acc: 0.9310 - val_loss: 0.5339 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00111: val_acc did not improve\n",
      "Epoch 112/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2041 - acc: 0.9313 - val_loss: 0.5335 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00112: val_acc did not improve\n",
      "Epoch 113/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2036 - acc: 0.9317 - val_loss: 0.5342 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00113: val_acc did not improve\n",
      "Epoch 114/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2031 - acc: 0.9314 - val_loss: 0.5343 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00114: val_acc did not improve\n",
      "Epoch 115/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2025 - acc: 0.9319 - val_loss: 0.5349 - val_acc: 0.8281\n",
      "\n",
      "Epoch 00115: val_acc did not improve\n",
      "Epoch 116/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2020 - acc: 0.9327 - val_loss: 0.5344 - val_acc: 0.8268\n",
      "\n",
      "Epoch 00116: val_acc did not improve\n",
      "Epoch 117/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2014 - acc: 0.9323 - val_loss: 0.5361 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00117: val_acc did not improve\n",
      "Epoch 118/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2009 - acc: 0.9324 - val_loss: 0.5355 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00118: val_acc did not improve\n",
      "Epoch 119/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2003 - acc: 0.9335 - val_loss: 0.5361 - val_acc: 0.8263\n",
      "\n",
      "Epoch 00119: val_acc did not improve\n",
      "Epoch 120/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.2000 - acc: 0.9333 - val_loss: 0.5370 - val_acc: 0.8261\n",
      "\n",
      "Epoch 00120: val_acc did not improve\n",
      "Epoch 121/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1992 - acc: 0.9331 - val_loss: 0.5362 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00121: val_acc did not improve\n",
      "Epoch 122/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1989 - acc: 0.9342 - val_loss: 0.5355 - val_acc: 0.8272\n",
      "\n",
      "Epoch 00122: val_acc did not improve\n",
      "Epoch 123/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1981 - acc: 0.9343 - val_loss: 0.5358 - val_acc: 0.8286\n",
      "\n",
      "Epoch 00123: val_acc did not improve\n",
      "Epoch 124/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1979 - acc: 0.9337 - val_loss: 0.5372 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00124: val_acc did not improve\n",
      "Epoch 125/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1972 - acc: 0.9347 - val_loss: 0.5376 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00125: val_acc did not improve\n",
      "Epoch 126/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1968 - acc: 0.9348 - val_loss: 0.5370 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00126: val_acc did not improve\n",
      "Epoch 127/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1963 - acc: 0.9349 - val_loss: 0.5370 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00127: val_acc did not improve\n",
      "Epoch 128/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1958 - acc: 0.9344 - val_loss: 0.5383 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00128: val_acc did not improve\n",
      "Epoch 129/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1954 - acc: 0.9352 - val_loss: 0.5386 - val_acc: 0.8254\n",
      "\n",
      "Epoch 00129: val_acc did not improve\n",
      "Epoch 130/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1948 - acc: 0.9349 - val_loss: 0.5393 - val_acc: 0.8261\n",
      "\n",
      "Epoch 00130: val_acc did not improve\n",
      "Epoch 131/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1945 - acc: 0.9355 - val_loss: 0.5382 - val_acc: 0.8261\n",
      "\n",
      "Epoch 00131: val_acc did not improve\n",
      "Epoch 132/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1939 - acc: 0.9360 - val_loss: 0.5381 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00132: val_acc did not improve\n",
      "Epoch 133/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1935 - acc: 0.9363 - val_loss: 0.5388 - val_acc: 0.8254\n",
      "\n",
      "Epoch 00133: val_acc did not improve\n",
      "Epoch 134/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1929 - acc: 0.9366 - val_loss: 0.5394 - val_acc: 0.8267\n",
      "\n",
      "Epoch 00134: val_acc did not improve\n",
      "Epoch 135/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1925 - acc: 0.9361 - val_loss: 0.5392 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00135: val_acc did not improve\n",
      "Epoch 136/1000\n",
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.1921 - acc: 0.9362 - val_loss: 0.5387 - val_acc: 0.8282\n",
      "\n",
      "Epoch 00136: val_acc did not improve\n",
      "Epoch 137/1000\n",
      "32488/32488 [==============================] - 2s 73us/step - loss: 0.1916 - acc: 0.9367 - val_loss: 0.5413 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00137: val_acc did not improve\n",
      "Epoch 138/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1912 - acc: 0.9365 - val_loss: 0.5401 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00138: val_acc did not improve\n",
      "Epoch 139/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1907 - acc: 0.9376 - val_loss: 0.5392 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00139: val_acc did not improve\n",
      "Epoch 140/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1903 - acc: 0.9374 - val_loss: 0.5397 - val_acc: 0.8272\n",
      "\n",
      "Epoch 00140: val_acc did not improve\n",
      "Epoch 141/1000\n",
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.1897 - acc: 0.9378 - val_loss: 0.5418 - val_acc: 0.8272\n",
      "\n",
      "Epoch 00141: val_acc did not improve\n",
      "Epoch 142/1000\n",
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.1894 - acc: 0.9382 - val_loss: 0.5411 - val_acc: 0.8254\n",
      "\n",
      "Epoch 00142: val_acc did not improve\n",
      "Epoch 143/1000\n",
      "32488/32488 [==============================] - 2s 72us/step - loss: 0.1889 - acc: 0.9385 - val_loss: 0.5398 - val_acc: 0.8279\n",
      "\n",
      "Epoch 00143: val_acc did not improve\n",
      "Epoch 144/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1884 - acc: 0.9385 - val_loss: 0.5433 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00144: val_acc did not improve\n",
      "Epoch 145/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1881 - acc: 0.9382 - val_loss: 0.5414 - val_acc: 0.8263\n",
      "\n",
      "Epoch 00145: val_acc did not improve\n",
      "Epoch 146/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1876 - acc: 0.9388 - val_loss: 0.5412 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00146: val_acc did not improve\n",
      "Epoch 147/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1873 - acc: 0.9388 - val_loss: 0.5429 - val_acc: 0.8274\n",
      "\n",
      "Epoch 00147: val_acc did not improve\n",
      "Epoch 148/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1867 - acc: 0.9387 - val_loss: 0.5427 - val_acc: 0.8251\n",
      "\n",
      "Epoch 00148: val_acc did not improve\n",
      "Epoch 149/1000\n",
      "32488/32488 [==============================] - 2s 71us/step - loss: 0.1864 - acc: 0.9392 - val_loss: 0.5422 - val_acc: 0.8258\n",
      "\n",
      "Epoch 00149: val_acc did not improve\n",
      "Epoch 00149: early stopping\n"
     ]
    }
   ],
   "source": [
    "H = model_top.fit(train_data, train_labels,\n",
    "          epochs=epochs, batch_size=batch_size, \n",
    "          callbacks = [checkpoint, early],\n",
    "          validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "N = 149\n",
    "plt.plot(np.arange(365, N+365), H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(365, N+365), H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.savefig('bottleneck.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
